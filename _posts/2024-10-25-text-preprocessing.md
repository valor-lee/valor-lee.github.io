---
title: 텍스트 전처리(Text Pre-Processing)
date: 2024-10-25 20:29:00 +09:00
categories: [Deep Learning]
tags:
  [
    Deep Learning, 
    text preprocess,
    tokenization
  ]
---


# 개요

전체적인 흐름

코퍼스 데이터에서 문장 토큰화를 진행하고 정제작업과 정규화 작업을 진행합니다. 이 후 문장 토큰 별 단어 토큰화하고 소문자화하여 단어 개수를 통일하고 불용어들을 제외시킵니다. 이와 같은 전처리를 마친 후, 각 문장 토큰들에 대해서 인덱싱 작업이 진행됩니다.


# 토큰화에서 고려해야할 사항

1. 구두점이나 특수 문자를 단순 제외해서는 안 된다.
2. 줄임말과 단어 내에 띄어쓰기가 있는 경우


# 한국어 tokenizer

- KSS(Korean Sentence Splitter)


# 한국어 토큰화 어려움

1. 교착어

한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 형태소 토큰화를 수행해야 한다.
- 자립형태소와 의존형태소로 구분하여 토큰화해야 한다는 의미입니다.

2. 띄어쓰기

한국어 코퍼스는 띄어쓰기가 잘 지켜지지 않는 경우가 많다.

3. 품사 태깅(part of speech tagging)

단어의 품사에 따라 의미가 달라진다.
- 영어도 마찬가지이긴 함

단어 토큰화할 떄 각 단어가 어떤 품사로 사용되었는지 구분해놓아야 함.


# 한국어 자연어 처리 패키지

KoNLPy
- 파이썬 패키지

## 형태소 분석기

- Okt(Open Korea Text)
- 메캅(Mecab)
- 코모란(Komoran)
- 한나눔(Hannanum)
- 꼬꼬마(Kkma) 

# 어간 추출(Stemming)

## 표제어 추출(Lemmatization)


표제어(Lemma) : 한글로는 '표제어' 또는 '기본 사전형 단어' 

- am, are, is의 표제어는 be

해당 단어의 품사 정보를 보존한다.

형태학적 파싱

- 표제어 추출에 가장 섬세한 방식
- 어간(stem), 접사(affix)를 분리함


WordNetLemmatizer
- 표제어 추출 도구
- 정확한 파싱을 위해 단어의 품사를 알려줄 필요가 있음

## 어간 추출(Stemming)

정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업


포터 알고리즘
- NLTK 에서 지원
- 마틴 포터에서 상세 규칙 확인

랭커스터 스태머 알고리즘
- NLTK 에서 지원


어간 추출이 빠르지만 표제어 추출 만큼 섬세하지 않아보임

## 한국어 어간 추출

| 언 | 품사 |
| --- | --- | 
| 체언 | 명사, 대명사, 수사 |
| 수식언 | 	관형사, 부사 |
| 관계언 |	조사 |
독립언 |	감탄사 |
| 용언 |	동사, 형용사 |


용언에 해당하는 동사와 형용사는 어간과 어미의 결합으로 구성됨
 
활용(Conjugation)
- 용언의 어간이 어미를 가지는 일
- 어간의 형태가 변함 여부에 따라 불/규칙적 활용으로 나뉜다.


출처 : https://namu.wiki/w/한국어/불규칙%20활용


# 불용어(stopword)

분석에 큰 의미가 없는 단어

NLTK 에서 개발자가 불용어를 추가해서 제거 가능하다.

그냥 토큰들 for loop문 돌면서 불용어리스트에 포함되는지 확인하는 방법(nested loop)

불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt 파일이나 csv 파일로 정리해놓고 이를 불러와서 사용한다.

링크 : https://www.ranks.nl/stopwords/korean

# 정규 표현식

## 텍스트 전처리 모듈

`re`

- 정규표현식 모듈
- 특정 규칙이 있는 텍스트 데이터를 빠르게 정제 가능하다

관련함수
- 문자열을 컴파일한 정규표현식으로 쉽게 가공할 수 있도록 제공된다.

`NLTK.RegexpTokenizer`
- 정규표현식을 사용한 단어 토큰화 함수
- 하나의 토큰으로 규정하기를 원하는 정규 표현식을 인자로 받아서 토큰화 수행

- [정규 표현식 정리](https://wikidocs.net/21703)

# 정수 인코딩

단어에 인덱스를 부여하는 작업

텍스트를 수치화하기 전에 최대한 전처리를 마쳐야 한다.

인덱싱은 랜덤 또는 빈도수 기준으로 부여될 수 있다.

## 빈도수 기준 정수 인코딩

높은 빈도의 토큰만 사용하고 싶을 때 쓸 수 있는 방법이다.

아래와 같은 절차를 거친다.
- dictionary 자료형을 사용하여 빈도수 계산한다.
- 각 토큰의 빈도수를 저장하고 높은 빈도의 단어부터 낮은 인덱스를 부여한다.
- 낮은 빈도수의 토큰에 대해서 제거하는 작업을 수행할 수 있다.
  - 이 과정에 따라 Out Of Vocabulary 문제가 발생할 수 있기 때문에 OOV 인덱스를 추가해준다.
- 문장 토큰들을 인덱스로 대체한다.

## 정수 인코딩을 지원하는 함수

Counter, FreqDist 사용하거나, 케라스 토크나이저를 사용할 수 있다.

### Counter, NLTK.FreqDist

dictionary 자료형 사용한 빈도수 계산 로직이 함수화되어 있다.

### 케라스 토크나이저

tokenizer의 fit_on_texts 함수를 통해 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여한다.
- word_index, word_counts, texts_to_sequences 로 확인 가능

기본적으로 OOV 문제에 대해서 아예 단어를 지워버린다.

보존하고 싶다면 Tokenizer의 인자 oov_token 옵션을 지정할 수 있다.


# 패딩(Padding)

기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있다.

병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업을 한다.

Numpy, 

## Numpy 기반 zero padding

인코딩된 문장 토큰의 길이가 가장 긴 값의 길이가 되도록 모든 문장 토큰에 0(PAD) 값을 추가한다.
- np가 딱히 하는 건 없다

## Keras 기반 padding

pad_sequences 함수로 앞에 0으로 체운다.

padding='post' 인자로 뒤에 0을 채울 수 있다.

maxlen 인자로 문서의 패딩 길이를 제한 할 수 있다.

- 기존 문서가 길었다면 데이터 손실이 발생할 수 있다.

  - truncating='post' 인자로 뒷 데이터라 손실되도록 할 수 있다.

value 인자로 padding value를 지정할 수 있다.

# 원-핫 인코딩(One-Hot Encoding)

단어 집합을 벡터화하는 인코딩 방법입니다.

단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다.

keras.utils.to_categorical로 간편이 원핫 인코딩을 구성할 수 있다.

벡터 공간이 늘어난다는 단점이 있다.
- 단어 집합의 크기가 곧 벡터 차원의 수가 된다.
  - 1000개의 단어로 구성된 코퍼스로 인코딩하면 각 단어별로 1000 차원을 차지한다.
단어의 유사도를 표현하지 못한다.
- 검색 시스템에서 문제가 될 수 있다. 유사 단어에 대한 연관검색이 불가능하다는 뜻이다.

위 단점을 보완하기위해 

 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다.
 
 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재합니다.


# 데이터의 분리(Splitting Data)

지도학습 테스트 데이터에서 x, y를 분리해야 하거나, 이미 분리된 x, y에서 테스트 데이터를 분리해야 하는 작업이 필요합니다.


## 1. 테스트 데이터에서 x, y를 분리

학습의 정확도를 테스트하기 위해 x_test, y_test를 분리해야 합니다.

### zip 함수를 이용하여 분리

`x, y = zip(*sequences)`

### 데이터프레임을 이용하여 분리

`df = pd.DataFrame(values, columns=columns)`

### Numpy를 이용하여 분리

```python

np_array = np.arange(0,16).reshape((4,4))
X = np_array[:, :3]
y = np_array[:,3]
```

## 이미 분리된 x, y에서 테스트 데이터를 분리


`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)`

# 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)

 KoNLPy와 KSS(Korean Sentence Splitter)와 함께 유용하게 사용할 수 있는 패키지들입니다.

## PyKoSpacing

띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지

대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있습니다.

## Py-Hanspell

네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지

현재 Py-hanspell은 제대로 동작하지 않습니다. (2024/03/03)

## SOYNLP를 이용한 단어 토큰화

품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저입니다.

비지도 학습으로 단어 토큰화를 한다는 특징이 있습니다.
- 다수의 한국어 문서를 학습하여 전체 코퍼스에 대한 단어 점수표를 생성합니다.
- 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용한 단어 점수표로 토큰화합니다.

신조어가 포함되어 있는 코퍼스에 효과적입니다.

- 신조어 문제 : 자주 등장하는 단어의 경우 한 단어로 파악할 수 있습니다.

### SOYNLP의 응집 확률(cohesion probability)

내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도입니다.

응집 확률은 문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 왼쪽부터 순서대로 문자를 추가하면서 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값입니다. 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높습니다.

- "반포한강공원에" 라는 문자열에서 "반보한" 또는 "반포한강공원에" 문자열보다 응집률이 높다는 뜻입니다.

### SOYNLP의 브랜칭 엔트로피(branching entropy)

주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지를 판단하는 척도입니다.

문자 시퀀스에서 다음 문자 예측을 위해 헷갈리는 정도로 이해하면 됩니다.

- 브랜칭 엔트로피의 값은 하나의 완성된 단어에 가까워질수록 정확히 예측할 수 있게 되면서 값이 낮아지는 양상을 보입니다.


### SOYNLP의 L tokenizer

한국어는 띄어쓰기 단위로 나눈 어절 토큰은 주로 L 토큰 + R 토큰의 형식을 가질 때가 많습니다.

L 토큰 + R 토큰으로 나누되, 분리 기준을 응집 확률 점수가 가장 높은 L 토큰을 찾아내는 원리를 가지고 있습니다.

### 최대 점수 토크나이저

띄어쓰기가 되지 않는 문장에서 응집 확률 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저입니다.


## SOYNLP를 이용한 반복되는 문자 정제

emoticon_normalize, repeat_normalize로 불필요한 반복 문자를 제거할 수 있습니다.

## Customized KoNLPy

형태소를 개발자가 지정해줄 수 있습니다.

`pip install customized_konlpy`




